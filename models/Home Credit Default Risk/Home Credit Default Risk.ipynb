{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mvahit/anaconda3/envs/home_credit/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import seaborn as sns\n",
    "from feature_engine import categorical_encoders as ce\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from pandas import DataFrame\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\",\n",
    "                                                                                                   ascending=False)[\n",
    "           :100].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    print(best_features)\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')\n",
    "\n",
    "\n",
    "def load_dataset(file_path, index=0):\n",
    "    df = pd.read_csv(file_path, index_col=index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_categoric_columns(df):\n",
    "    cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    return cols\n",
    "\n",
    "\n",
    "def apply_label_encoding(l_df, columns):\n",
    "    lbe = LabelEncoder()\n",
    "\n",
    "    for col in columns:\n",
    "        l_df[col] = lbe.fit_transform(l_df[col])\n",
    "\n",
    "    return l_df\n",
    "\n",
    "\n",
    "def apply_one_hot_encoding(l_df):\n",
    "    original_columns = list(l_df)  # col names as string in a list\n",
    "    categorical_columns = get_categoric_columns(l_df)  # categorical col names\n",
    "    l_df = pd.get_dummies(l_df, columns=categorical_columns, drop_first=True)  # creating dummies\n",
    "    new_columns = [c for c in l_df.columns if c not in original_columns]  # new col names\n",
    "    return l_df, new_columns\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def rare_encoding(data, variables, rare_threshold=0.05, n_rare_categories=4):\n",
    "    encoder = ce.RareLabelCategoricalEncoder(tol=rare_threshold, n_categories=n_rare_categories, variables=variables,\n",
    "                                             replace_with='Rare')\n",
    "    # fit the encoder\n",
    "    encoder.fit(data)\n",
    "\n",
    "    # transform the data\n",
    "    data = encoder.transform(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "            to reduce memory usage.\n",
    "        \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    # code takenn from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_early_shutdown(row):\n",
    "    early_shutdown = 0\n",
    "    if row.CREDIT_ACTIVE == \"Active\" and row.DAYS_CREDIT_ENDDATE < 0:\n",
    "        early_shutdown = 1\n",
    "    return early_shutdown\n",
    "\n",
    "\n",
    "def quasi_constant(data,threshold = 0.98):\n",
    "    # create empty list\n",
    "    quasi_constant_feature = []\n",
    "              \n",
    "    # loop over all the columns\n",
    "    for feature in data.columns:\n",
    "    \n",
    "         # calculate the ratio.\n",
    "        predominant = (data[feature].value_counts() / np.float(len(data))).sort_values(ascending=False).values[0]\n",
    "          \n",
    "         # append the column name if it is bigger than the threshold\n",
    "        if predominant >= threshold:\n",
    "             quasi_constant_feature.append(feature)   \n",
    " \n",
    "    return quasi_constant_feature\n",
    "\n",
    "# Check for duplicate feature in data\n",
    "def duplicated_features(data):\n",
    "      duplicated_feat = []\n",
    "    \n",
    "      for i in range(0,len(data.columns)):\n",
    "            if i % 10 == 0: # will show the loop\n",
    "                print(i)\n",
    "            col_1 = data.columns[i]\n",
    "            \n",
    "            for col_2 in data.columns[i + 1:]:\n",
    "                if(data[col_1].equals(data[col_2])):\n",
    "                    duplicated_feat.append(col_2) \n",
    "      \n",
    "      return duplicated_feat\n",
    "\n",
    "# find and remove correlated features\n",
    "def correlation(data, threshold):\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = data.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    col_corr.add(colname)\n",
    "        return col_corr    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buro_add_feature(df_breau):\n",
    "    df_bureau_new = pd.DataFrame()\n",
    "    # kredi başvuru sayısı\n",
    "    df_bureau_new[\"BURO_CREDIT_APPLICATION_COUNT\"] = df_breau.groupby(\"SK_ID_CURR\").count()[\"SK_ID_BUREAU\"]\n",
    "\n",
    "    # aktif kredi sayısı\n",
    "    df_bureau_new[\"BURO_ACTIVE_CREDIT_APPLICATION_COUNT\"] = \\\n",
    "        df_breau[df_breau[\"CREDIT_ACTIVE\"] == \"Active\"].groupby(\"SK_ID_CURR\").count()[\"CREDIT_ACTIVE\"]\n",
    "    df_bureau_new[\"BURO_ACTIVE_CREDIT_APPLICATION_COUNT\"].fillna(0, inplace=True)\n",
    "\n",
    "    # pasif kredi sayısı\n",
    "    df_bureau_new[\"BURO_CLOSED_CREDIT_APPLICATION_COUNT\"] = \\\n",
    "        df_breau[df_breau[\"CREDIT_ACTIVE\"] == \"Closed\"].groupby(\"SK_ID_CURR\").count()[\"CREDIT_ACTIVE\"]\n",
    "    df_bureau_new[\"BURO_CLOSED_CREDIT_APPLICATION_COUNT\"].fillna(0, inplace=True)\n",
    "\n",
    "    # erken kredi kapama\n",
    "    df_bureau_new[\"BURO_EARLY_SHUTDOWN_NEW\"] = df_breau.apply(lambda x: feature_early_shutdown(x), axis=1)\n",
    "\n",
    "    # geciktirilmiş ödeme sayısı\n",
    "    df_bureau_new[\"BURO_NUMBER_OF_DELAYED_PAYMENTS\"] = \\\n",
    "        df_breau[df_breau[\"AMT_CREDIT_MAX_OVERDUE\"] != 0].groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_MAX_OVERDUE\"].count()\n",
    "    df_bureau_new[\"BURO_NUMBER_OF_DELAYED_PAYMENTS\"].fillna(0, inplace=True)\n",
    "\n",
    "    # son kapanmış başvurusu üzerinden geçen max süre\n",
    "    df_bureau_new[\"BURO_MAX_TIME_PASSED_CREDIT_APPLICATION\"] = \\\n",
    "        df_breau[df_breau[\"CREDIT_ACTIVE\"] == \"Closed\"].groupby(\"SK_ID_CURR\")[\"DAYS_ENDDATE_FACT\"].max()\n",
    "    df_bureau_new[\"BURO_MAX_TIME_PASSED_CREDIT_APPLICATION\"].fillna(0, inplace=True)\n",
    "\n",
    "    # geciktirilmiş max ödeme tutari\n",
    "    df_bureau_new[\"BURO_MAX_DELAYED_PAYMENTS\"] = df_breau.groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_MAX_OVERDUE\"].max()\n",
    "    df_bureau_new[\"BURO_MAX_DELAYED_PAYMENTS\"].fillna(0, inplace=True)\n",
    "\n",
    "    # geciktirilmiş ödeyenlerden oluşan top liste - en yüksek 100\n",
    "    # gecikme olan (80302, 12)\n",
    "    df_bureau_new[\"BURO_DELAYED_PAYMENTS_TOP_100_NEW\"] = \\\n",
    "        df_bureau_new.sort_values(\"BURO_MAX_DELAYED_PAYMENTS\", ascending=False)[\"BURO_MAX_DELAYED_PAYMENTS\"].rank()\n",
    "    df_bureau_new[\"BURO_DELAYED_PAYMENTS_TOP_100_NEW\"].fillna(0, inplace=True)\n",
    "\n",
    "    # kredi uzatma yapilmis mi\n",
    "    df_bureau_new[\"BURO_IS_CREDIT_EXTENSION_NEW\"] = df_breau.groupby(\"SK_ID_CURR\")[\"CNT_CREDIT_PROLONG\"].count().apply(\n",
    "        lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # max yapilan kredi uzatmasi\n",
    "    df_bureau_new[\"BURO_CREDIT_EXTENSION_MAX\"] = df_breau.groupby(\"SK_ID_CURR\")[\"CNT_CREDIT_PROLONG\"].max()\n",
    "    df_bureau_new[\"BURO_CREDIT_EXTENSION_MAX\"].fillna(0, inplace=True)\n",
    "\n",
    "    # unsuccessful credit payment - borç takarak kapanmış kredi ödemeleri tespit et\n",
    "    df_bureau_new[\"BURO_IS_UNSUCCESSFUL_CREDIT_PAYMENT_NEW\"] = \\\n",
    "        df_breau[(df_breau[\"CREDIT_ACTIVE\"] == \"Closed\") & (df_breau[\"AMT_CREDIT_SUM_DEBT\"] > 0)].groupby(\n",
    "            \"SK_ID_CURR\").all()[\"AMT_CREDIT_SUM_DEBT\"].apply(lambda x: 1 if x == True else 0)\n",
    "    df_bureau_new[\"BURO_IS_UNSUCCESSFUL_CREDIT_PAYMENT_NEW\"].fillna(0, inplace=True)\n",
    "\n",
    "    return df_bureau_new\n",
    "\n",
    "\n",
    "def load_data_with_application_train(num_rows=None):\n",
    "    df_app_train = application_train()\n",
    "    print(\"application_train df shape:\", df_app_train.shape)\n",
    "    bureau, bureau_add_features = bureau_and_balance()\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    bureau = bureau.fillna(0)\n",
    "    return df_app_train, bureau, bureau_add_features\n",
    "\n",
    "\n",
    "def load_data_only_bureau_and_bureau_balance(num_rows=None):\n",
    "    bureau, bureau_add_features = bureau_and_balance()\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    bureau = bureau.fillna(0)\n",
    "    return bureau, bureau_add_features\n",
    "\n",
    "\n",
    "def app_train_bureau_merge(num_rows=None):\n",
    "    df_app_train, bureau, bureau_add_features = load_data_with_application_train(num_rows)\n",
    "    # df_merge = pd.merge(df_app_train, bureau, on=['SK_ID_CURR'],how='inner')\n",
    "    df_merge = bureau\n",
    "    # print(\"app_train, bureau merge shape:\", df_merge.shape)\n",
    "    print(\"bureau merge shape:\", df_merge.shape)\n",
    "    df_final = pd.merge(df_merge, bureau_add_features, on=['SK_ID_CURR'], how='inner')\n",
    "    print(\"Bureau add features df shape:\", bureau_add_features.shape)\n",
    "    del df_app_train, bureau, bureau_add_features, df_merge\n",
    "    gc.collect()\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def bureau_and_bureau_balance_features(num_rows=None):\n",
    "    bureau, bureau_add_features = load_data_only_bureau_and_bureau_balance(num_rows)\n",
    "    df_final = pd.merge(bureau, bureau_add_features, on=['SK_ID_CURR'], how='inner')\n",
    "    print(\"Bureau add features df shape:\", bureau_add_features.shape)\n",
    "    del bureau, bureau_add_features\n",
    "    gc.collect()\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def application_train():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df_app_train = pd.read_sql_query(\"SELECT * FROM application_train\", conn)\n",
    "    df_app_train = df_app_train[[\"TARGET\", \"SK_ID_CURR\"]]\n",
    "    # df_app_train = df_app_train.dropna()\n",
    "    df_app_train.reset_index(drop=True, inplace=True)\n",
    "    gc.collect()\n",
    "    return df_app_train\n",
    "\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(nan_as_category=True):\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    bureau = pd.read_sql_query(\"SELECT * FROM bureau\", conn)\n",
    "    bb = pd.read_sql_query(\"SELECT * FROM bureau_balance\", conn)\n",
    "    bureau[\"AMT_CREDIT_SUM_DEBT\"] = bureau[\"AMT_CREDIT_SUM_DEBT\"].fillna(0)\n",
    "    bureau.fillna(0, inplace=True)\n",
    "    bb.fillna(0, inplace=True)\n",
    "    # bureau = bureau.dropna()\n",
    "    bureau.reset_index(drop=True, inplace=True)\n",
    "    # bb = bb.dropna()\n",
    "    bb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # add_features\n",
    "    bureau_add_features = buro_add_feature(df_breau=bureau)\n",
    "\n",
    "    # sum agg b_balance\n",
    "    # Status_sum ile ilgili yeni bir degisken olusturma\n",
    "    bb_dummy = pd.get_dummies(bb, dummy_na=True)\n",
    "    agg_list = {\"MONTHS_BALANCE\": \"count\",\n",
    "                \"STATUS_0\": [\"sum\"],\n",
    "                \"STATUS_1\": [\"sum\"],\n",
    "                \"STATUS_2\": [\"sum\"],\n",
    "                \"STATUS_3\": [\"sum\"],\n",
    "                \"STATUS_4\": [\"sum\"],\n",
    "                \"STATUS_5\": [\"sum\"],\n",
    "                \"STATUS_C\": [\"sum\"],\n",
    "                \"STATUS_X\": [\"sum\"]}\n",
    "    bb_sum_agg = bb_dummy.groupby(\"SK_ID_BUREAU\").agg(agg_list)\n",
    "    # Degisken isimlerinin yeniden adlandirilmasi\n",
    "    bb_sum_agg.columns = pd.Index([\"BURO_\" + col[0] + \"_\" + col[1].upper() for col in bb_sum_agg.columns.tolist()])\n",
    "    # Status_sum ile ilgili yeni bir degisken olusturma\n",
    "    bb_sum_agg['BURO_NEW_STATUS_SCORE'] = bb_sum_agg['BURO_STATUS_1_SUM'] + bb_sum_agg['BURO_STATUS_2_SUM'] ^ 2 + \\\n",
    "                                          bb_sum_agg['BURO_STATUS_3_SUM'] ^ 3 + bb_sum_agg['BURO_STATUS_4_SUM'] ^ 4 + \\\n",
    "                                          bb_sum_agg['BURO_STATUS_5_SUM'] ^ 5\n",
    "    bb_sum_agg.drop(\n",
    "        ['BURO_STATUS_1_SUM', 'BURO_STATUS_2_SUM', 'BURO_STATUS_3_SUM', 'BURO_STATUS_4_SUM', 'BURO_STATUS_5_SUM'],\n",
    "        axis=1, inplace=True)\n",
    "\n",
    "    # CREDIT_TYPE degiskeninin sinif sayisini 3'e düsürmek\n",
    "    bureau['CREDIT_TYPE'] = bureau['CREDIT_TYPE'].replace(['Car loan',\n",
    "                                                           'Mortgage',\n",
    "                                                           'Microloan',\n",
    "                                                           'Loan for business development',\n",
    "                                                           'Another type of loan',\n",
    "                                                           'Unknown type of loan',\n",
    "                                                           'Loan for working capital replenishment',\n",
    "                                                           \"Loan for purchase of shares (margin lending)\",\n",
    "                                                           'Cash loan (non-earmarked)',\n",
    "                                                           'Real estate loan',\n",
    "                                                           \"Loan for the purchase of equipment\",\n",
    "                                                           \"Interbank credit\",\n",
    "                                                           \"Mobile operator loan\"], 'Rare')\n",
    "\n",
    "    # CREDIT_ACTIVE degiskeninin sinif sayisini 2'ye düsürmek (Sold' u Closed a dahil etmek daha mi uygun olur ???)\n",
    "    bureau['CREDIT_ACTIVE'] = bureau['CREDIT_ACTIVE'].replace(['Bad debt', 'Sold'], 'Active')\n",
    "\n",
    "    # one hot encoding start\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    # one hot encoding end\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size'],\n",
    "                       \"STATUS_0\": [\"mean\"],\n",
    "                       \"STATUS_C\": [\"mean\"],\n",
    "                       \"STATUS_X\": [\"mean\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "\n",
    "    # b_balance sum değşkenlerinin eklenmesi\n",
    "    bb_agg[\"BURO_MONTHS_BALANCE_COUNT\"] = bb_sum_agg[\"BURO_MONTHS_BALANCE_COUNT\"]\n",
    "    bb_agg[\"BURO_STATUS_0_SUM\"] = bb_sum_agg[\"BURO_STATUS_0_SUM\"]\n",
    "    bb_agg[\"BURO_STATUS_C_SUM\"] = bb_sum_agg[\"BURO_STATUS_C_SUM\"]\n",
    "    bb_agg[\"BURO_STATUS_X_SUM\"] = bb_sum_agg[\"BURO_STATUS_X_SUM\"]\n",
    "    bb_agg[\"BURO_NEW_STATUS_SCORE\"] = bb_sum_agg[\"BURO_NEW_STATUS_SCORE\"]\n",
    "\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    bureau[\"BURO_MONTHS_BALANCE_COUNT\"].fillna(0, inplace=True)\n",
    "    bureau[\"BURO_STATUS_0_SUM\"].fillna(0, inplace=True)\n",
    "    bureau[\"BURO_STATUS_C_SUM\"].fillna(0, inplace=True)\n",
    "    bureau[\"BURO_STATUS_X_SUM\"].fillna(0, inplace=True)\n",
    "    bureau[\"BURO_NEW_STATUS_SCORE\"].fillna(0, inplace=True)\n",
    "\n",
    "    ##ek son değişkenler\n",
    "    # ortalama kac aylık kredi aldıgını gösteren yeni degisken\n",
    "    bureau[\"BURO_NEW_MONTHS_CREDIT\"] = round((bureau.DAYS_CREDIT_ENDDATE - bureau.DAYS_CREDIT) / 30)\n",
    "\n",
    "    bureau.drop(columns='SK_ID_BUREAU', inplace=True)\n",
    "\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum', 'std'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum', 'std', 'median'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'DAYS_CREDIT_UPDATE': ['min', 'max', 'mean'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['BURO_ACT_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['BURO_CLS_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg, bureau_add_features\n",
    "\n",
    "\n",
    "def application_train_g():\n",
    "    print(\"application_tarin_g\")\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df = pd.read_sql_query(\"SELECT * FROM application_train\", conn)\n",
    "    test_df = pd.read_sql_query(\"SELECT * FROM application_test\", conn)\n",
    "    print(\"connection finished\")\n",
    "    df = reduce_mem_usage(df)\n",
    "    test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = le.fit_transform(df[\"NAME_EDUCATION_TYPE\"])\n",
    "    df.loc[(df[\"NAME_EDUCATION_TYPE\"] == 1), \"NAME_EDUCATION_TYPE\"] = 0\n",
    "\n",
    "    df.loc[(df[\"CNT_FAM_MEMBERS\"] > 3), \"CNT_FAM_MEMBERS\"] = 4\n",
    "\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "    lbe = LabelEncoder()\n",
    "\n",
    "    for col in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[col] = lbe.fit_transform(df[col])\n",
    "\n",
    "    # df = pd.get_dummies(df, dummy_na = True)\n",
    "\n",
    "    nom_list = [\n",
    "        'EMERGENCYSTATE_MODE',\n",
    "        'FONDKAPREMONT_MODE',\n",
    "        'HOUSETYPE_MODE',\n",
    "        'NAME_CONTRACT_TYPE',\n",
    "        'NAME_FAMILY_STATUS',\n",
    "        'NAME_HOUSING_TYPE',\n",
    "        'NAME_INCOME_TYPE',\n",
    "        'NAME_TYPE_SUITE',\n",
    "        'OCCUPATION_TYPE',\n",
    "        'ORGANIZATION_TYPE',\n",
    "        'WALLSMATERIAL_MODE',\n",
    "        'WEEKDAY_APPR_PROCESS_START']\n",
    "\n",
    "    df = rare_encoding(df, nom_list)\n",
    "    df = pd.get_dummies(df, columns=nom_list, drop_first=True)\n",
    "\n",
    "    # new_features\n",
    "    # 1\n",
    "    df[\"NEW_GOODSPRICE/CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
    "    # 2\n",
    "    df[\"NEW_ANNUITY/CREDIT\"] = (df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"])\n",
    "    # 3\n",
    "    df[\"NEW_INCOME/ANNUITY\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_ANNUITY\"]\n",
    "    # 4\n",
    "    df[\"NEW_DAYS_LAST_PHONE_CHANGE\"] = df[\"DAYS_LAST_PHONE_CHANGE\"]\n",
    "    df.loc[(df[\"NEW_DAYS_LAST_PHONE_CHANGE\"] == 0), \"NEW_DAYS_LAST_PHONE_CHANGE\"] = 1\n",
    "    df.loc[(df[\"NEW_DAYS_LAST_PHONE_CHANGE\"] != 0), \"NEW_DAYS_LAST_PHONE_CHANGE\"] = 0\n",
    "    # 5\n",
    "    df[\"DAYS_BIRTH\"] = df[\"DAYS_BIRTH\"] / 365\n",
    "    df[\"NEW_DAYS_BIRTH\"] = df[\"DAYS_BIRTH\"]\n",
    "    df.loc[(df[\"NEW_DAYS_BIRTH\"] <= 41), \"NEW_DAYS_BIRTH\"] = 1\n",
    "    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 41), \"NEW_DAYS_BIRTH\"] = 0\n",
    "    # 6\n",
    "    df[\"NEW_CREDIT/INCOME\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    # 7\n",
    "    df[\"NEW_WORK/NOTWORK\"] = df[\"DAYS_EMPLOYED\"]\n",
    "    df.loc[(df[\"NEW_WORK/NOTWORK\"] == 0), \"NEW_WORK/NOTWORK\"] = 0  # ÇALIŞMAYANLAR\n",
    "    df.loc[(df[\"NEW_WORK/NOTWORK\"] != 0), \"NEW_WORK/NOTWORK\"] = 1  # ÇALIŞANLAR\n",
    "    # 8\n",
    "    df[\"NEW_INCOME/CREDIT\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    # 9\n",
    "    # En yakın zaman (soruşturma olmayan 0, saat+gün+ hafta+ay için 1, ay+yıl için 2)\n",
    "    df[\"NEW_REQ\"] = df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"]\n",
    "    # yakın ve orta zamanda soruşturma\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] > 0), \"NEW_REQ\"] = 1\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] > 0), \"NEW_REQ\"] = 1\n",
    "\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] == 0) & (\n",
    "            df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"] > 0), \"NEW_REQ\"] = 1\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] == 0) & (\n",
    "            df[\"AMT_REQ_CREDIT_BUREAU_MON\"] > 0), \"NEW_REQ\"] = 1\n",
    "    # uzak zaman soruşturma\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] == 0) &\n",
    "           (df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_MON\"] == 0) &\n",
    "           (df[\"AMT_REQ_CREDIT_BUREAU_QRT\"] > 0), \"NEW_REQ\"] = 2\n",
    "\n",
    "    df.loc[(df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] == 0) &\n",
    "           (df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"] == 0) & (df[\"AMT_REQ_CREDIT_BUREAU_MON\"] == 0) &\n",
    "           (df[\"AMT_REQ_CREDIT_BUREAU_YEAR\"] > 0), \"NEW_REQ\"] = 2\n",
    "    # soruşturma olmayanlar\n",
    "    df.loc[(pd.isna(df[\"NEW_REQ\"])), \"NEW_REQ\"] = 0\n",
    "\n",
    "    # eski grup yeni feature ları\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "\n",
    "    df['NEW_EXT_RESOURCE_3_CREDIT_TO_GOODS_RATIO'] = df['EXT_SOURCE_3'] / (df['AMT_CREDIT'] / df['AMT_GOODS_PRICE'])\n",
    "    df['NEW_EXT_RESOURCE_2_CREDIT_TO_GOODS_RATIO'] = df['EXT_SOURCE_2'] / (df['AMT_CREDIT'] / df['AMT_GOODS_PRICE'])\n",
    "    df['NEW_EXT_RESOURCE_1_CREDIT_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] / (df['AMT_CREDIT'] / df['AMT_GOODS_PRICE'])\n",
    "    \n",
    "\n",
    "    df.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "    df.columns = pd.Index([\"APP_\" + col for col in df.columns.tolist()])\n",
    "\n",
    "    df.rename(columns={\"APP_SK_ID_CURR\": \"SK_ID_CURR\"}, inplace=True)\n",
    "\n",
    "    df.rename(columns={\"APP_TARGET\": \"TARGET\"}, inplace=True)\n",
    "    \n",
    "    print(\"DF SAYISI:\")\n",
    "    print(len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def previous_application():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df_prev = pd.read_sql_query(\"SELECT * FROM previous_application\", conn)\n",
    "    df_prev = reduce_mem_usage(df_prev)\n",
    "  \n",
    "\n",
    "    # Features that has outliers\n",
    "    feat_outlier = [\"AMT_ANNUITY\", \"AMT_APPLICATION\", \"AMT_CREDIT\", \"AMT_DOWN_PAYMENT\", \"AMT_GOODS_PRICE\",\n",
    "                    \"SELLERPLACE_AREA\"]\n",
    "\n",
    "    # Replacing the outliers of the features with their own upper values\n",
    "    for var in feat_outlier:\n",
    "        Q1 = df_prev[var].quantile(0.01)\n",
    "        Q3 = df_prev[var].quantile(0.99)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        df_prev[var][(df_prev[var] > upper)] = upper\n",
    "\n",
    "    # 365243 value will be replaced by NaN in the following features\n",
    "    feature_replace = ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE',\n",
    "                       'DAYS_TERMINATION']\n",
    "\n",
    "    for var in feature_replace:\n",
    "        df_prev[var].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # One hot encoding\n",
    "    categorical_columns = [col for col in df_prev.columns if df_prev[col].dtype == 'object']\n",
    "    #df_prev = pd.get_dummies(df_prev, columns=categorical_columns, dummy_na=True)\n",
    "\n",
    "    df_prev = df_prev.drop(categorical_columns, axis = 1) \n",
    "    \n",
    "    # Creating new features\n",
    "\n",
    "    df_prev['APP_CREDIT_PERC'] = df_prev['AMT_APPLICATION'] / df_prev['AMT_CREDIT']\n",
    "    df_prev['NEW_CREDIT_TO_ANNUITY_RATIO'] = df_prev['AMT_CREDIT'] / df_prev['AMT_ANNUITY']\n",
    "    df_prev['NEW_DOWN_PAYMENT_TO_CREDIT'] = df_prev['AMT_DOWN_PAYMENT'] / df_prev['AMT_CREDIT']\n",
    "    df_prev['NEW_TOTAL_PAYMENT'] = df_prev['AMT_ANNUITY'] * df_prev['CNT_PAYMENT']\n",
    "    df_prev['NEW_TOTAL_PAYMENT_TO_AMT_CREDIT'] = df_prev['NEW_TOTAL_PAYMENT'] / df_prev['AMT_CREDIT']\n",
    "    # Innterest ratio previous application (simplified)\n",
    "\n",
    "    df_prev['SIMPLE_INTERESTS'] = (df_prev['NEW_TOTAL_PAYMENT'] / df_prev['AMT_CREDIT'] - 1) / df_prev['CNT_PAYMENT']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {}\n",
    "    num_cols = df_prev.select_dtypes(exclude=['object'])\n",
    "    num_cols.drop(['SK_ID_PREV', 'SK_ID_CURR'], axis=1, inplace=True)\n",
    "\n",
    "    for num in num_cols:\n",
    "        num_aggregations[num] = ['min', 'max', 'mean', 'var', 'sum']\n",
    "\n",
    "        # Previous applications categoric features\n",
    "    cat_aggregations = {}\n",
    "    for i in df_prev.columns:\n",
    "        if df_prev[i].dtypes == \"O\":\n",
    "            cat_aggregations[i] = ['mean']\n",
    "\n",
    "    prev_agg = df_prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    # Dropping features with small variance\n",
    "    features_with_small_variance = prev_agg.columns[(prev_agg.std(axis=0) < .1).values]\n",
    "    prev_agg.drop(features_with_small_variance, axis=1, inplace=True)\n",
    "    prev_agg.reset_index(inplace=True)\n",
    "\n",
    "    return prev_agg\n",
    "\n",
    "\n",
    "def previous_application_b():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df_control=reduce_mem_usage(pd.read_sql_query('SELECT * FROM home_credit.previous_application',conn))\n",
    "    \n",
    "    #en son yapilan basvuruya göre filtreledik\n",
    "    df_days=df_control.loc[df_control.groupby('SK_ID_CURR')['DAYS_DECISION'].idxmax()]\n",
    "    df_days = df_days[['SK_ID_CURR','DAYS_DECISION','NAME_CONTRACT_STATUS']]\n",
    "    #LAST_APPEAL kolonu olarak degistirdik aslinda buna gerek yok feature importance da da yan yanalar gene de koydum\n",
    "    df_days=df_days.rename(columns={'DAYS_DECISION': 'LAST_APPEAL','NAME_CONTRACT_STATUS': 'STATUS_OF_LAST_APPEAL'})\n",
    "    AMOUNT_OF_APPEAL = [] \n",
    "    AMOUNT_OF_APPEAL=df_control.groupby('SK_ID_CURR')['NAME_CONTRACT_STATUS'].apply(lambda y: (y=='Approved').sum()).reset_index(name='count')\n",
    "    AMOUNT_OF_APPEAL=AMOUNT_OF_APPEAL.rename(columns={'count': 'AMT_APPROVED'})\n",
    "    AMOUNT_OF_TOTAL_APPEAL=df_control.groupby(['SK_ID_CURR']).count().sort_values('SK_ID_PREV', ascending=False)  \n",
    "    AMOUNT_OF_TOTAL_APPEAL=AMOUNT_OF_TOTAL_APPEAL.reset_index().sort_values(by='SK_ID_CURR', ascending=True)\n",
    "    AMOUNT_OF_TOTAL_APPEAL=AMOUNT_OF_TOTAL_APPEAL.rename(columns={'SK_ID_PREV': 'AMOUNT_OF_TOTAL_APPEAL'})\n",
    "    AMOUNT_OF_TOTAL_APPEAL = AMOUNT_OF_TOTAL_APPEAL[['SK_ID_CURR', 'AMOUNT_OF_TOTAL_APPEAL']]\n",
    "    AMOUNT_OF_mean_total=df_control.groupby('SK_ID_CURR')['AMT_APPLICATION'].sum().reset_index(name='count')\n",
    "    AMOUNT_OF_mean_approved=df_control.groupby('SK_ID_CURR')['AMT_CREDIT'].sum().reset_index(name='count') \n",
    "    df_control_last=df_control.sort_values(by='SK_ID_CURR', ascending=True)\n",
    "    df_control_last=df_control_last.drop_duplicates(subset=['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    AMOUNT_OF_mean_total=AMOUNT_OF_mean_total.rename(columns={'count': 'AMOUNT_OF_mean_total'})\n",
    "    AMOUNT_OF_mean_approved=AMOUNT_OF_mean_approved.rename(columns={'count': 'AMOUNT_OF_mean_approved'})\n",
    "    \n",
    "    ############################################################\n",
    "    \n",
    "    df_result = pd.merge(AMOUNT_OF_mean_approved, AMOUNT_OF_mean_total, on='SK_ID_CURR') \n",
    "\n",
    "    df_result = pd.merge(df_result, AMOUNT_OF_TOTAL_APPEAL, on='SK_ID_CURR') \n",
    "    \n",
    "    df_result = pd.merge(df_result, AMOUNT_OF_APPEAL, on='SK_ID_CURR') \n",
    "    \n",
    "    df_result = pd.merge(df_result, df_days, on='SK_ID_CURR') \n",
    "    \n",
    "    df_result['APPROVED_CREDIT/TOTAL_CREDIT'] = df_result['AMOUNT_OF_mean_approved']/df_result['AMOUNT_OF_mean_total'] \n",
    "    \n",
    "    df_result['AMT_APPROVED/TOTAL'] = df_result['AMT_APPROVED']/df_result['AMOUNT_OF_TOTAL_APPEAL'] \n",
    "    #burada app train den veri cekmek zorundayiz ! veri cektigim sorgu calismayabilir, alttaki satir yani\n",
    "    \n",
    "    \n",
    "   ############################################################\n",
    "    \n",
    "    \n",
    "    print(\"Credit Application\")\n",
    "    df_target= reduce_mem_usage(pd.read_sql_query('SELECT * FROM home_credit.application_train',conn))\n",
    "    print(\"Credit App bağlandı\")\n",
    "\n",
    "    df_target = df_target[['SK_ID_CURR', 'TARGET']]\n",
    "    df_ml = pd.merge(df_result, df_target,how='right', on='SK_ID_CURR')\n",
    "    df_ml = pd.merge(df_ml, df_control_last,how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    print(\"installments_payments\")\n",
    "    df_install = reduce_mem_usage(pd.read_sql_query('SELECT * FROM home_credit.installments_payments',conn))\n",
    "    print(\"installments_payments bağlandı\")\n",
    "\n",
    "    df_install['Paying_Rate'] = df_install['AMT_PAYMENT']/df_install['AMT_INSTALMENT']\n",
    "    df_install['Delaying_Number'] = df_install['DAYS_ENTRY_PAYMENT']-df_install['DAYS_INSTALMENT']\n",
    "    df_install=df_install.drop_duplicates(subset=['SK_ID_CURR'])\n",
    "    df_ml = pd.merge(df_ml, df_install,how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    df_ml_verison2=df_ml.replace('XAP',np.NaN)\n",
    "    df_ml_verison2=df_ml_verison2.replace('XNA',np.NaN)\n",
    "    df_ml_verison2=df_ml_verison2.replace('XAP',np.NaN)\n",
    "    df_ml_verison3 = df_ml_verison2.dropna(subset=['NAME_CONTRACT_TYPE','APPROVED_CREDIT/TOTAL_CREDIT','NAME_CLIENT_TYPE','PRODUCT_COMBINATION','CHANNEL_TYPE'])\n",
    "\n",
    "    \n",
    "    def rare_encoding_sum(df, variable,threshold):\n",
    "\n",
    "        list1=[]\n",
    "        for var in df[variable].unique(): \n",
    "            list1.append(var) \n",
    "        for x in range(len(list1)): \n",
    "            if df[variable].value_counts(dropna=False)[list1[x]]*100/len(df[variable]) <= threshold :\n",
    "                    df[variable].replace({list1[x]: \"Rare\"}, inplace=True)\n",
    "\n",
    "                    \n",
    "    \n",
    "     \n",
    "        \n",
    "    # cok fazla kategoriler icin yuzde5 rare kategori icine toplayabildiklerimizi topladik\n",
    "    rare_encoding_sum(df_ml_verison3,'PRODUCT_COMBINATION',1.8)\n",
    "    rare_encoding_sum(df_ml_verison3,'NAME_YIELD_GROUP',2)\n",
    "    rare_encoding_sum(df_ml_verison3,'NAME_SELLER_INDUSTRY',2.14)\n",
    "    rare_encoding_sum(df_ml_verison3,'NAME_GOODS_CATEGORY',1.4) \n",
    "    rare_encoding_sum(df_ml_verison3,'CODE_REJECT_REASON',2.5)\n",
    "    rare_encoding_sum(df_ml_verison3,'NAME_CASH_LOAN_PURPOSE',1.5)\n",
    "    rare_encoding_sum(df_ml_verison3,'NAME_PAYMENT_TYPE',1)\n",
    "  \n",
    "    #lambda x: x*10 if x<2 else (x**2 if x<4 else x+10)\n",
    "    df_ml_verison3['Delaying_Number-CAT'] = df_ml_verison3['Delaying_Number'].apply(lambda x:'Paid Earlier' if x < 0 else ( 'Paid in Time' if x == 0 else 'Paid Later') )\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"AMT_APPROVED\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"AMOUNT_OF_mean_approved\", axis=1)\n",
    "    df_ml_verison3['started_on_time'] = df_ml_verison3['DAYS_FIRST_DUE']-df_ml_verison3['DAYS_LAST_DUE_1ST_VERSION']\n",
    "    df_ml_verison3['started_on_time-CAT'] = df_ml_verison3['started_on_time'].apply(lambda x:'Paid Later' if x < 0 else ( 'Paid in Time' if x == 0 else 'Paid Earlier') )\n",
    "    df_ml_verison3['finished_on_time'] = df_ml_verison3['DAYS_LAST_DUE']-df_ml_verison3['DAYS_TERMINATION']\n",
    "    df_ml_verison3['finished_on_time-CAT'] = df_ml_verison3['finished_on_time'].apply(lambda x:'Paid Later' if x < 0 else ( 'Paid in Time' if x == 0 else 'Paid Earlier') )\n",
    "    #gereksiz featurelar atiliyor\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"started_on_time\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"finished_on_time\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"DAYS_FIRST_DUE\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"DAYS_LAST_DUE_1ST_VERSION\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"DAYS_LAST_DUE\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"DAYS_TERMINATION\", axis=1)\n",
    "    #df_ml_verison3=df_ml_verison3.drop(\"SK_ID_CURR\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"TARGET\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"SK_ID_PREV_x\", axis=1)\n",
    "    df_ml_verison3=df_ml_verison3.drop(\"SK_ID_PREV_y\", axis=1)\n",
    "    #sayilarda cok ufak bir nan cikti modele sokabilmek icin atiyorum\n",
    "    df_ml_verison3=df_ml_verison3.dropna(subset=['Delaying_Number', 'Paying_Rate','AMT_PAYMENT','DAYS_ENTRY_PAYMENT','DAYS_INSTALMENT','NUM_INSTALMENT_NUMBER','NUM_INSTALMENT_VERSION'])\n",
    "    cols_to_use=[]\n",
    "    for col in df_ml_verison3.columns: \n",
    "        cols_to_use.append(col) \n",
    "\n",
    "\n",
    "    #kategoriklerde nan value lari atmak istemiyorum missing koydum \n",
    "    # Now we impute the missing values with SimpleImputer\n",
    "\n",
    "    # create an instance of the simple imputer\n",
    "    # we indicate that we want to impute by replacing NA\n",
    "    # with 'Missing'\n",
    "\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value = 'Missing')\n",
    "\n",
    "    # we fit the imputer to the train set\n",
    "    # the imputer will learn the median of all variables\n",
    "    imputer.fit(df_ml_verison3)   # SIL\n",
    "    #has prev appl\n",
    "    #not prev apl\n",
    "    df_ml_verison3=imputer.transform(df_ml_verison3)\n",
    "    df_ml_verison3 = pd.DataFrame(df_ml_verison3, columns=cols_to_use)\n",
    "    df_ml_verison3=df_ml_verison3.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    df_ml_verison3['Paying_Rate'] = df_ml_verison3['Paying_Rate'].fillna(0)\n",
    "\n",
    "    nom_list=['FLAG_LAST_APPL_PER_CONTRACT','NAME_CASH_LOAN_PURPOSE','NAME_SELLER_INDUSTRY','PRODUCT_COMBINATION','WEEKDAY_APPR_PROCESS_START','CHANNEL_TYPE',\n",
    "     'NAME_GOODS_CATEGORY','NAME_PORTFOLIO','NAME_CONTRACT_TYPE','NAME_PAYMENT_TYPE','NAME_TYPE_SUITE']\n",
    "    df_ml_verison3=pd.get_dummies(df_ml_verison3, columns=nom_list, drop_first=True)\n",
    "    ord_list_V3=['STATUS_OF_LAST_APPEAL', 'NAME_CONTRACT_STATUS', 'NAME_YIELD_GROUP', 'finished_on_time-CAT','Delaying_Number-CAT' ,'started_on_time-CAT', 'NAME_PRODUCT_TYPE' ,'CODE_REJECT_REASON', 'NAME_CLIENT_TYPE']\n",
    "\n",
    "    for i in ord_list_V3:\n",
    "        labelencoder = LabelEncoder()\n",
    "        le = LabelEncoder()\n",
    "        df_ml_verison3[i] = le.fit_transform(df_ml_verison3[i])\n",
    "\n",
    "    \n",
    "    #alinacak featurelar yukaridadir\n",
    "    gc.collect()\n",
    "    return df_ml_verison3\n",
    "\n",
    "\n",
    "def credit_card_balance():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    ccb = pd.read_sql_query(\"SELECT * FROM credit_card_balance\", conn)\n",
    "\n",
    "    ccb = reduce_mem_usage(ccb)\n",
    "\n",
    "    ccb = ccb.groupby('SK_ID_CURR').agg(['mean'])\n",
    "    e = 0\n",
    "    ccb.columns = pd.Index(\n",
    "        ['CC_' + ccb.columns[e][0] + \"_\" + ccb.columns[e][1].upper() for e in range(ccb.columns.size)])\n",
    "\n",
    "    # new feature1: calculating the rate of balance(loan) to the credit card limit\n",
    "    ccb[\"CC_NEW_LOAN_TO_CREDIT_LIMIT_RATE\"] = (ccb[\"CC_AMT_BALANCE_MEAN\"] + 1) / (\n",
    "            ccb[\"CC_AMT_CREDIT_LIMIT_ACTUAL_MEAN\"] + 1)\n",
    "\n",
    "    # new feature2: at what rate the customer paid the loan:CC_AMT_PAYMENT_TOTAL_CURRENT_MEAN /\n",
    "    # CC_AMT_TOTAL_RECEIVABLE_MEAN: CC_PAID_AMOUNT_RATE\n",
    "    ccb[\"CC_NEW_PAID_AMOUNT_RATE\"] = (ccb[\"CC_AMT_PAYMENT_TOTAL_CURRENT_MEAN\"] + 1) / (\n",
    "            ccb[\"CC_AMT_TOTAL_RECEIVABLE_MEAN\"] + 1) * 100\n",
    "\n",
    "    # new feature3: how much money the customer withdrew in avg from ATM per drawing:AMOUNT PER ATM DRAWING\n",
    "    ccb[\"CC_NEW_AMT_PER_ATM_DRAWING_MEAN\"] = (ccb[\"CC_AMT_DRAWINGS_ATM_CURRENT_MEAN\"] + 1) / (\n",
    "            ccb[\"CC_CNT_DRAWINGS_ATM_CURRENT_MEAN\"] + 1)\n",
    "\n",
    "    # new feature4: how much money the customer withdrew from POS in avg per drawing:AMOUNT PER POS DRAWING\n",
    "    ccb[\"CC_NEW_AMT_PER_POS_DRAWING_MEAN\"] = (ccb[\"CC_AMT_DRAWINGS_POS_CURRENT_MEAN\"] + 1) / (\n",
    "            ccb[\"CC_CNT_DRAWINGS_POS_CURRENT_MEAN\"] + 1)\n",
    "\n",
    "    ccb = pd.concat([ccb.loc[:, \"CC_NEW_LOAN_TO_CREDIT_LIMIT_RATE\"],\n",
    "                     ccb.loc[:, \"CC_NEW_PAID_AMOUNT_RATE\"],\n",
    "                     ccb.loc[:, \"CC_AMT_CREDIT_LIMIT_ACTUAL_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_AMT_PAYMENT_CURRENT_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_MONTHS_BALANCE_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_CNT_INSTALMENT_MATURE_CUM_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_AMT_INST_MIN_REGULARITY_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_AMT_DRAWINGS_ATM_CURRENT_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_AMT_DRAWINGS_POS_CURRENT_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_CNT_DRAWINGS_ATM_CURRENT_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_CNT_DRAWINGS_POS_CURRENT_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_NEW_AMT_PER_ATM_DRAWING_MEAN\"],\n",
    "                     ccb.loc[:, \"CC_NEW_AMT_PER_POS_DRAWING_MEAN\"]], axis=1)\n",
    "\n",
    "    return ccb\n",
    "\n",
    "\n",
    "def prepare_instalment_payment():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df_installments_payments = pd.read_sql_query(\"SELECT * FROM installments_payments\", conn)\n",
    "\n",
    "    df_installments_payments = reduce_mem_usage(df_installments_payments)\n",
    "\n",
    "    # O anki taksitin yuzde kaci odendi\n",
    "    df_installments_payments[['AMT_PAYMENT']] = df_installments_payments[['AMT_PAYMENT']].fillna(value=0)\n",
    "    df_installments_payments['NEW_INSTALMENT_PAYMENT_RATE'] = df_installments_payments['AMT_PAYMENT'] / \\\n",
    "                                                              df_installments_payments['AMT_INSTALMENT'] * 100\n",
    "\n",
    "    # O anki taksit son odeme gununden kac gun once odenmis. Bu degisken \"NEW_INSTALMENT_PAYMENT_STATUS\" degerini bulabilmek icin gecici olusturulur.\n",
    "    df_installments_payments['NEW_DAY_BEFORE_END_DATE'] = df_installments_payments['DAYS_INSTALMENT'] - \\\n",
    "                                                          df_installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "\n",
    "    df_installments_payments[\"NEW_INSTALMENT_PAYMENT_STATUS\"] = \"No Payment\"\n",
    "    df_installments_payments.loc[\n",
    "        df_installments_payments['NEW_DAY_BEFORE_END_DATE'] == 0, \"NEW_INSTALMENT_PAYMENT_STATUS\"] = \"In Time\"\n",
    "    df_installments_payments.loc[\n",
    "        df_installments_payments['NEW_DAY_BEFORE_END_DATE'] > 0, \"NEW_INSTALMENT_PAYMENT_STATUS\"] = \"Early\"\n",
    "    df_installments_payments.loc[\n",
    "        df_installments_payments['NEW_DAY_BEFORE_END_DATE'] < 0, \"NEW_INSTALMENT_PAYMENT_STATUS\"] = \"Late\"\n",
    "\n",
    "    df_installments_payments[\"NEW_INS_IS_LATE\"] = \"No\"\n",
    "    df_installments_payments.loc[df_installments_payments['NEW_DAY_BEFORE_END_DATE'] < 0, \"NEW_INS_IS_LATE\"] = \"Yes\"\n",
    "    # Iki siniftan olustugu icin LabelEncoding yapilir.\n",
    "    df_installments_payments = apply_label_encoding(df_installments_payments, [\"NEW_INS_IS_LATE\"])\n",
    "\n",
    "    df_installments_payments.drop(columns=['NEW_DAY_BEFORE_END_DATE'], inplace=True)\n",
    "\n",
    "    df_installments_payments, ip_cat = apply_one_hot_encoding(df_installments_payments)\n",
    "\n",
    "    ip_aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['max'],\n",
    "        'NUM_INSTALMENT_NUMBER': ['max'],\n",
    "        'AMT_INSTALMENT': ['sum'],\n",
    "        'AMT_PAYMENT': ['sum'],\n",
    "        'NEW_INSTALMENT_PAYMENT_RATE': ['min', 'max', 'mean'],\n",
    "        'NEW_INS_IS_LATE': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for col in ip_cat:\n",
    "        ip_aggregations[col] = ['mean']\n",
    "\n",
    "    df_ip_agg = df_installments_payments.groupby(['SK_ID_CURR']).agg(ip_aggregations)\n",
    "\n",
    "    df_ip_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_ip_agg.columns.tolist()])\n",
    "\n",
    "    return df_ip_agg\n",
    "\n",
    "\n",
    "def prepare_pos_cash_balance():\n",
    "    conn = pymysql.connect(host='35.228.28.142', port=int(63306), user='group2', passwd='123654', db='home_credit')\n",
    "    df_pos_cash_balance = pd.read_sql_query(\"SELECT * FROM POS_CASH_balance\", conn)\n",
    "\n",
    "    df_pos_cash_balance, pcb_cat = apply_one_hot_encoding(df_pos_cash_balance)\n",
    "\n",
    "    pcb_aggregations = {\n",
    "        'SK_ID_PREV': ['min', 'max', 'mean', 'count'],\n",
    "        'MONTHS_BALANCE': ['min', 'max'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "\n",
    "    for col in pcb_cat:\n",
    "        pcb_aggregations[col] = ['mean']\n",
    "\n",
    "    df_pcb_agg = df_pos_cash_balance.groupby(['SK_ID_CURR']).agg(pcb_aggregations)\n",
    "    df_pcb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_pcb_agg.columns.tolist()])\n",
    "\n",
    "    return df_pcb_agg\n",
    "\n",
    "\n",
    "def installment_payment_main():\n",
    "    df_ip_agg = prepare_instalment_payment()  # Intaslment Payment son hali\n",
    "\n",
    "    df_pcb_agg = prepare_pos_cash_balance()  # Pos cash balance son hali\n",
    "\n",
    "    df_pos_ins = df_ip_agg.join(df_pcb_agg, how='inner',\n",
    "                                on=['SK_ID_CURR'])  # instalment payment ve pos cash balance birlestirilmis hali\n",
    "\n",
    "    return df_ip_agg, df_pcb_agg, df_pos_ins\n",
    "\n",
    "\n",
    "def pre_processing_and_combine():\n",
    "    \n",
    "    with timer(\"Process application train\"):\n",
    "        df = application_train_g()\n",
    "        print(\"application train & test shape:\", df.shape)\n",
    "\n",
    "    values = set(df.SK_ID_CURR)\n",
    "    \n",
    "    with timer(\"previous_application\"):\n",
    "        prev_agg_bk = previous_application_b()\n",
    "        prev_agg_bk.columns = ['BK_'+col if col != 'SK_ID_CURR'  else col for col in prev_agg_bk.columns]\n",
    "        print(\"previous_application_bk:\", prev_agg_bk.shape)  \n",
    "    \n",
    "    df['NEW_HAS_APPLICATION'] = prev_agg_bk.SK_ID_CURR.isin(values).astype(int)\n",
    "    df['NEW_HAS_APPLICATION'].fillna(0,inplace=True)    \n",
    "    df['NEW_HAS_APPLICATION'].head()\n",
    "    \n",
    "    with timer(\"previous_application\"):\n",
    "        prev_agg = previous_application()\n",
    "        print(\"previous_application:\", prev_agg.shape)     \n",
    " \n",
    "    with timer(\"Bureau and Bureau Balance\"):\n",
    "        df_final = bureau_and_bureau_balance_features()\n",
    "        print(\"Bureau and Bureau Balance:\", df_final.shape)\n",
    "        \n",
    "    with timer(\"Installment Payments\"):\n",
    "        df_ip_agg, df_pcb_agg, df_pos_ins = installment_payment_main()\n",
    "        print(\"Installment Payments\", df_ip_agg.shape)\n",
    "\n",
    "    with timer(\"Pos Cash Balance\"):\n",
    "        print(\"Pos Cash Balance:\", df_pcb_agg.shape)\n",
    "\n",
    "    with timer(\"Credit Card Balance\"):\n",
    "        ccb = credit_card_balance()\n",
    "        print(\"Credit Card Balance:\", ccb.shape)\n",
    "        \n",
    "        \n",
    "  \n",
    "    cbb_index = DataFrame(ccb.index.values.tolist())\n",
    "    df['NEW_FLAG_CCB'] = cbb_index.isin(values).astype(int)\n",
    "    df['NEW_FLAG_CCB'].fillna(0,inplace=True)\n",
    "    df['NEW_FLAG_CCB'].head()\n",
    "    \n",
    "    with timer(\"All tables are combining\"):\n",
    "       \n",
    "        df1 = df.merge(df_final, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        df2 = df1.merge(df_pos_ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        #df2 = df1.merge(df_ip_agg, how='left', on='SK_ID_CURR')\n",
    "        #df3 = df2.merge(df_pcb_agg, how='left', on='SK_ID_CURR')\n",
    "        df4 = df2.merge(ccb, how='left', on='SK_ID_CURR')\n",
    "        df5 = df4.merge(prev_agg_bk, how='left', on='SK_ID_CURR')\n",
    "        all_df = df5.merge(prev_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        print(\"all_df process:\", all_df.shape)\n",
    " \n",
    "    \n",
    "    return all_df\n",
    "\n",
    "\n",
    "def modeling(all_data):\n",
    "    all_data = all_data.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "    train_df = all_data[all_data['TARGET'].notnull()]\n",
    "    test_df = all_data[all_data['TARGET'].isnull()]\n",
    "\n",
    "    folds = KFold(n_splits=10, shuffle=True, random_state=1001)\n",
    "\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR']]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            n_jobs=-1,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=40,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=12,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric='auc', verbose=100, early_stopping_rounds=200)\n",
    "\n",
    "        # y_pred_valid\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))  # y_pred_valid\n",
    "\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv(\"atilla_muhammet.csv'\", index=False)\n",
    "\n",
    "    display_importances(feature_importance_df)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with timer(\"Preprocessing Time\"):\n",
    "        all_data = pre_processing_and_combine()\n",
    "        all_data.to_csv('export_homecredit.csv', index = False, header=True)\n",
    "\n",
    "    #with timer(\"Modeling\"):\n",
    "    #    feat_importance = modeling(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_tarin_g\n",
      "connection finished\n",
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 14.60 MB\n",
      "Decreased by 67.6%\n",
      "DF SAYISI:\n",
      "356250\n",
      "application train & test shape: (356250, 163)\n",
      "Process application train - done in 104s\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "Credit Application\n",
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Credit App bağlandı\n",
      "installments_payments\n",
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "installments_payments bağlandı\n",
      "previous_application_bk: (278136, 97)\n",
      "previous_application - done in 930s\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "previous_application: (338857, 96)\n",
      "previous_application - done in 181s\n",
      "Bureau df shape: (305812, 120)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SK_ID_CURR'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-a7b13a15e84c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtimer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Full model run\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-2ccd4c59a59a>\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtimer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Preprocessing Time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m         \u001B[0mall_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpre_processing_and_combine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m         \u001B[0mall_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'export_homecredit.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-5992c8e9acdd>\u001B[0m in \u001B[0;36mpre_processing_and_combine\u001B[0;34m()\u001B[0m\n\u001B[1;32m    732\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    733\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtimer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Bureau and Bureau Balance\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 734\u001B[0;31m         \u001B[0mdf_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbureau_and_bureau_balance_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    735\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Bureau and Bureau Balance:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_final\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-5992c8e9acdd>\u001B[0m in \u001B[0;36mbureau_and_bureau_balance_features\u001B[0;34m(num_rows)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbureau_and_bureau_balance_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_rows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[0mbureau\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbureau_add_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_data_only_bureau_and_bureau_balance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_rows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m     \u001B[0mdf_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbureau\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbureau_add_features\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'SK_ID_CURR'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'inner'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Bureau add features df shape:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbureau_add_features\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0;32mdel\u001B[0m \u001B[0mbureau\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbureau_add_features\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/home_credit/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36mmerge\u001B[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mindicator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindicator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mvalidate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m     )\n\u001B[1;32m     89\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/home_credit/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m    650\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mright_join_keys\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 652\u001B[0;31m         ) = self._get_merge_keys()\n\u001B[0m\u001B[1;32m    653\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m         \u001B[0;31m# validate the merge keys dtypes. We may need to coerce\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/home_credit/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m_get_merge_keys\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1003\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_rkey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1004\u001B[0m                         \u001B[0;32mif\u001B[0m \u001B[0mrk\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1005\u001B[0;31m                             \u001B[0mright_keys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mright\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1006\u001B[0m                         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                             \u001B[0;31m# work-around for merge_asof(right_index=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/home_credit/lib/python3.7/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m_get_label_or_level_values\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1558\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1559\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1560\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1561\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1562\u001B[0m         \u001B[0;31m# Check for duplicates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'SK_ID_CURR'"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = reduce_mem_usage(pd.read_csv('export_homecredit.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(export_df, 0.90)\n",
    "print('correlated features: ', len(set(corr_features)) )\n",
    "export_df.drop(labels=corr_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Burda Kesersek 79.415\n",
    "modeling(export_df)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}